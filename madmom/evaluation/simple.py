#!/usr/bin/env python
# encoding: utf-8
"""
This file contains basic evaluation functionality used by cp.evaluation modules.

@author: Sebastian BÃ¶ck <sebastian.boeck@jku.at>

"""

import numpy as np

from .helpers import calc_errors


def calc_overlap(detections, targets, threshold=0.5):
    """
    Very simple overlap calculation based on two numpy array of the same shape.
    The arrays should be a quantized version of any event lists.

    :param detections: array with detections
    :param targets:    array with targets
    :param threshold:  binary decision threshold [default=0.5]

    """
    # detections and targets must have the same dimensions
    if detections.size != targets.size:
        raise ValueError("dimension mismatch")
    if detections.ndim > 1:
        # TODO: implement for multi-dimensional arrays
        raise NotImplementedError("please add multi-dimensional functionality")
    # threshold detections
    detections = detections >= threshold
    # threshold targets
    targets = targets >= threshold
    # calculate overlap
    tp = np.nonzero(detections * targets)[0]
    fp = np.nonzero(detections > targets)[0]
    tn = np.nonzero(-detections * -targets)[0]
    fn = np.nonzero(detections < targets)[0]
    assert tp.size + tn.size + fp.size + fn.size == detections.size, 'bad overlap calculation'
    # return
    return tp, tn, fp, fn


class SimpleEvaluation(object):
    """
    Simple evaluation class for calculating Precision, Recall and F-measure
    based on the numbers of true/false positive/negative detections.

    Note: so far, this class is only suitable for a 1-class evaluation problem.

    """
    def __init__(self, num_tp=0, num_fp=0, num_tn=0, num_fn=0):
        """
        Creates a new SimpleEvaluation object instance.

        :param num_tp: number of true positive detections
        :param num_fp: number of false positive detections
        :param num_tn: number of true negative detections
        :param num_fn: number of false negative detections

        """
        # use hidden variables, because the properties get overridden in subclasses
        self.__num_tp = num_tp
        self.__num_fp = num_fp
        self.__num_tn = num_tn
        self.__num_fn = num_fn

    @property
    def num_tp(self):
        """Number of true positive detections."""
        return self.__num_tp

    @property
    def num_fp(self):
        """Number of false positive detections."""
        return self.__num_fp

    @property
    def num_tn(self):
        """Number of true negative detections."""
        return self.__num_tn

    @property
    def num_fn(self):
        """Number of false negative detections."""
        return self.__num_fn

    @property
    def precision(self):
        """Precision."""
        # correct / retrieved
        if self.num_tp == 0:
            # FIXME: why is this hack still needed? If not, we get a
            # RuntimeWarning: invalid value encountered in double_scalars
            return 0
        return self.num_tp / np.float64(self.num_tp + self.num_fp)

    @property
    def recall(self):
        """Recall."""
        # correct / relevant
        if self.num_tp == 0:
            # FIXME: why is this hack still needed? If not, we get a
            # RuntimeWarning: invalid value encountered in double_scalars
            return 0
        return self.num_tp / np.float64(self.num_tp + self.num_fn)

    @property
    def fmeasure(self):
        """F-measure."""
        numerator = 2 * self.precision * self.recall
        if numerator == 0:
            # FIXME: why is this hack still needed? If not, we get a
            # RuntimeWarning: invalid value encountered in double_scalars
            return 0
        return numerator / np.float64(self.precision + self.recall)

    @property
    def accuracy(self):
        """Accuracy."""
        # acc: (TP + TN) / (TP + FP + TN + FN)
        numerator = self.num_tp + self.num_tn
        if numerator == 0:
            # FIXME: why is this hack still needed? If not, we get a
            # RuntimeWarning: invalid value encountered in double_scalars
            return 0
        return numerator / np.float64(self.num_fp + self.num_fn + self.num_tp + self.num_tn)

    @property
    def mean_error(self):
        """Mean of the errors."""
        # FIXME: is returning 0 ok?
        return 0.

    @property
    def std_error(self):
        """Standard deviation of the errors."""
        # FIXME: is returning 0 ok?
        return 0.

    @property
    def errors(self):
        """Errors."""
        # FIXME: is returning an empty list?
        return np.empty(0)

    def print_errors(self, tex=False):
        """
        Print errors.

        :param tex: output format to be used in .tex files [default=False]

        """
        # print the errors
        print '  targets: %5d correct: %5d fp: %4d fn: %4d p=%.3f r=%.3f f=%.3f' % (self.num_tp + self.num_fn, self.num_tp, self.num_fp, self.num_fn, self.precision, self.recall, self.fmeasure)
        print '  tpr: %.1f%% fpr: %.1f%% acc: %.1f%% mean: %.1f ms std: %.1f ms' % (self.recall * 100., (1 - self.precision) * 100., self.accuracy * 100., self.mean_error * 1000., self.std_error * 1000.)
        if tex:
            print "%i events & Precision & Recall & F-measure & True Positves & False Positives & Accuracy & Delay\\\\" % (self.num)
            print "tex & %.3f & %.3f & %.3f & %.3f & %.3f & %.3f %.1f\$\\pm\$%.1f\\,ms\\\\" % (self.precision, self.recall, self.fmeasure, self.true_positive_rate, self.false_positive_rate, self.accuracy, self.mean_error * 1000., self.std_error * 1000.)

    def __str__(self):
        return "%s p=%.3f r=%.3f f=%.3f" % (self.__class__, self.precision, self.recall, self.fmeasure)


class SumEvaluation(SimpleEvaluation):
    """
    Simple evaluation class for summing true/false positive/(negative)
    detections and calculate Precision, Recall and F-measure.

    """
    # inherit from Evaluation class, since this is basically the same
    # this class just sums all the attributes and evaluates accordingly
    def __init__(self, other=None):
        super(SumEvaluation, self).__init__()
        self.__num_tp = 0
        self.__num_fp = 0
        self.__num_tn = 0
        self.__num_fn = 0
        self.__errors = np.empty(0)
        # instance can be initialized with a Evaluation object
        if other:
            # add this object to self
            self += other

    # for adding an Evaluation object
    def __add__(self, other):
        #if isinstance(other, Evaluation):
        if issubclass(other.__class__, SimpleEvaluation):
            # extend
            self.__num_tp += other.num_tp
            self.__num_fp += other.num_fp
            self.__num_tn += other.num_tn
            self.__num_fn += other.num_fn
            self.__errors = np.append(self.__errors, other.errors)
            return self
        else:
            return NotImplemented

    @property
    def num_tp(self):
        """Number of true positive detections."""
        return self.__num_tp

    @property
    def num_fp(self):
        """Number of false positive detections."""
        return self.__num_fp

    @property
    def num_tn(self):
        """Number of true negative detections."""
        return self.__num_tn

    @property
    def num_fn(self):
        """Number of false negative detections."""
        return self.__num_fn

    @property
    def mean_error(self):
        """Mean of the errors."""
        if not self.__errors.any():
            return 0
        return np.mean(self.__errors)

    @property
    def std_error(self):
        """Standard deviation of the errors."""
        if not self.__errors.any():
            return 0
        return np.std(self.__errors)


class MeanEvaluation(SimpleEvaluation):
    """
    Simple class for averaging Precision, Recall and F-measure.

    """
    def __init__(self, other=None):
        """
        Creates a new MeanEvaluation object instance.

        """
        super(MeanEvaluation, self).__init__()
        # redefine most of the stuff
        self.__precision = np.empty(0)
        self.__recall = np.empty(0)
        self.__fmeasure = np.empty(0)
        self.__accuracy = np.empty(0)
        self.__mean = np.empty(0)
        self.__std = np.empty(0)
        self.__errors = np.empty(0)
        self.__num_tp = np.empty(0)
        self.__num_fp = np.empty(0)
        self.__num_tn = np.empty(0)
        self.__num_fn = np.empty(0)
        self.num = 0
        # instance can be initialized with a Evaluation object
        if other:
            # add this object to self
            self += other

    # for adding a OnsetEvaluation object
    def __add__(self, other):
        """
        Appends the scores of another SimpleEvaluation object to the respective
        arrays.

        :param other: SimpleEvaluation object

        """
        if issubclass(other.__class__, SimpleEvaluation):
            self.__precision = np.append(self.__precision, other.precision)
            self.__recall = np.append(self.__recall, other.recall)
            self.__fmeasure = np.append(self.__fmeasure, other.fmeasure)
            self.__accuracy = np.append(self.__accuracy, other.accuracy)
            self.__mean = np.append(self.__mean, other.mean_error)
            self.__std = np.append(self.__std, other.std_error)
            self.__errors = np.append(self.__errors, other.errors)
            self.__num_tp = np.append(self.__num_tp, other.num_tp)
            self.__num_fp = np.append(self.__num_fp, other.num_fp)
            self.__num_tn = np.append(self.__num_tn, other.num_tn)
            self.__num_fn = np.append(self.__num_fn, other.num_fn)
            self.num += 1
            return self
        else:
            return NotImplemented

    @property
    def num_tp(self):
        """Number of true positive detections."""
        return np.mean(self.__num_tp)

    @property
    def num_fp(self):
        """Number of false positive detections."""
        return np.mean(self.__num_fp)

    @property
    def num_tn(self):
        """Number of true negative detections."""
        return np.mean(self.__num_tn)

    @property
    def num_fn(self):
        """Number of false negative detections."""
        return np.mean(self.__num_fn)

    @property
    def precision(self):
        """Precision."""
        return np.mean(self.__precision)

    @property
    def recall(self):
        """Recall."""
        return np.mean(self.__recall)

    @property
    def fmeasure(self):
        """F-measure."""
        return np.mean(self.__fmeasure)

    @property
    def accuracy(self):
        """Accuracy."""
        return np.mean(self.__accuracy)

    @property
    def errors(self):
        """Errors."""
        return self.__errors

    @property
    def mean_error(self):
        """Mean of the errors."""
        return np.mean(self.__mean)

    @property
    def std_error(self):
        """Standard deviation of the errors."""
        return np.mean(self.__std)


# simple class for evaluation of Presicion, Recall, F-measure
class Evaluation(SimpleEvaluation):
    """
    Evaluation class for measuring Precision, Recall and F-measure.

    """
    def __init__(self, detections, targets, eval_function, **kwargs):
        """
        Creates a new Evaluation object instance.

        :param detections:    sequence of estimated detections [seconds]
        :param targets:       sequence of ground truth targets [seconds]
        :param eval_function: evaluation function (see below)

        The evaluation function can be any function which returns a tuple of 4
        numpy arrays containing the true/false positive and negative detections:
        ([true positive], [false positive], [true negative], [false negative])

        Note: All arrays can be multi-dimensional with events aligned on axis 0.
              Additional information in other columns/axes is not used.

        """
        # detections, targets and evaluation function
        self.__detections = detections
        self.__targets = targets
        self.__eval_function = eval_function
        # save additional arguments and pass them to the evaluation function
        self.__kwargs = kwargs
        # init some hidden variables as None, calculate them on demand
        self.__tp = None
        self.__fp = None
        self.__tn = None
        self.__fn = None
        self.__errors = None

    def _calc_tp_fp_tn_fn(self):
        """Perform basic evaluation."""
        self.__tp, self.__fp, self.__tn, self.__fn = self.__eval_function(self.__detections, self.__targets, **self.__kwargs)

    @property
    def tp(self):
        """True positive detections."""
        if self.__tp is None:
            self._calc_tp_fp_tn_fn()
        return self.__tp

    @property
    def num_tp(self):
        """Number of true positive detections."""
        return self.tp.shape[0]

    @property
    def fp(self):
        """False positive detections."""
        if self.__fp is None:
            self._calc_tp_fp_tn_fn()
        return self.__fp

    @property
    def num_fp(self):
        """Number of false positive detections."""
        return self.fp.shape[0]

    @property
    def tn(self):
        """True negative detections."""
        if self.__tn is None:
            self._calc_tp_fp_tn_fn()
        return self.__tn

    @property
    def num_tn(self):
        """Number of true negative detections."""
        return self.tn.shape[0]

    @property
    def fn(self):
        """False negative detections."""
        if self.__fn is None:
            self._calc_tp_fp_tn_fn()
        return self.__fn

    @property
    def num_fn(self):
        """Number of false negative detections."""
        return self.fn.shape[0]

    @property
    def errors(self):
        """
        Absolute errors of all true positive detections relative to the closest
        targets.

        """
        if self.__errors is None:
            if self.num_tp == 0:
                # FIXME: what is the error in case of no TPs
                self.__errors = np.empty(0)
            else:
                self.__errors = calc_errors(self.tp, self.__targets)
        return self.__errors

    @property
    def mean_error(self):
        """
        Mean of the absolute errors of all true positive detections relative to
        the closest targets.

        """
        if not self.errors.any():
            return 0
        return np.mean(self.errors)

    @property
    def std_error(self):
        """
        Standard deviation of the absolute errors of all true positive
        detections relative to the clostest targets.

        """
        if not self.errors.any():
            return 0
        return np.std(self.errors)


def parser():
    import argparse
    # define parser
    p = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description="""
    The script evaluates a file or folder with detections against a file or
    folder with targets. Extensions can be given to filter the detection and
    target file lists.

    """)
    # files used for evaluation
    p.add_argument('detections', help='file (or folder) with detections to be evaluated (files being filtered according to the -d argument)')
    p.add_argument('targets', nargs='*', help='file (or folder) with targets (files being filtered according to the -t argument)')
    # extensions used for evaluation
    p.add_argument('-d', dest='det_ext', action='store', default=None, help='extension of the detection files')
    p.add_argument('-t', dest='tar_ext', action='store', default=None, help='extension of the target files')
    # parameters for evaluation
    p.add_argument('--tex', action='store_true', help='format errors for use is .tex files')
    # verbose
    p.add_argument('-v', dest='verbose', action='count', help='increase verbosity level')
    # parse the arguments
    args = p.parse_args()
    # print the args
    if args.verbose >= 2:
        print args
    # return
    return args


def main():
    from ..utils.helpers import files, match_file, load_events

    # parse arguments
    args = parser()
    # get detection and target files
    det_files = files(args.detections, args.det_ext)
    if not args.targets:
        args.targets = args.detections
    tar_files = files(args.targets, args.tar_ext)

    # sum and mean counter for all files
    sum_counter = SumEvaluation()
    mean_counter = MeanEvaluation()
    # evaluate all files
    for det_file in det_files:
        # get the detections file
        detections = load_events(det_file)
        # do a mean evaluation with all matched target files
        me = MeanEvaluation()
        for f in match_file(det_file, tar_files, args.det_ext, args.tar_ext):
            # load the targets
            targets = load_events(f)
            # test with onsets (but use the beat detection window of 70ms)
            from .onsets import count_errors
            # add the Evaluation to mean evaluation
            me += Evaluation(detections, targets, count_errors, window=0.07)
            # process the next target file
        # print stats for each file
        if args.verbose:
            me.print_errors(args.tex)
        # add the resulting sum counter
        sum_counter += me
        mean_counter += me
        # process the next detection file
    # print summary
    print 'sum for %i files:' % (len(det_files))
    sum_counter.print_errors(args.tex)
    print 'mean for %i files:' % (len(det_files))
    mean_counter.print_errors(args.tex)

if __name__ == '__main__':
    main()
