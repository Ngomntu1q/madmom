#!/usr/bin/env python
# encoding: utf-8
"""
Evaluation script.

"""

import os
import sys
import argparse
import warnings

import numpy as np

from madmom.utils import search_files, match_file, load_events, combine_events
from madmom.evaluation import onsets, beats, notes, tempo, alignment


def main():
    """Evaluation script"""

    # define parser
    p = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter, description='''
    This program evaluates pairs of files containing annotations and
    detections.

    Please note that all available evaluation options have their individual
    help messages, e.g.

    $ evaluate onsets -h

    ''')
    # set some global defaults
    p.set_defaults(skip=0, combine=0, delay=0, load_fn=load_events)

    # add version
    p.add_argument('--version', action='version', version='evaluate')

    # add subparsers
    sub_parsers = p.add_subparsers(title='available evaluation options')
    # onset evaluation
    onsets.add_parser(sub_parsers)
    # beat evaluation
    beats.add_parser(sub_parsers)
    # tempo evaluation
    tempo.add_parser(sub_parsers)
    # note evaluation
    notes.add_parser(sub_parsers)
    # alignment evaluation
    # alignment.add_parser(sub_parsers)

    # parse the args
    args = p.parse_args()

    # print the arguments
    if args.verbose >= 2:
        print args
    if args.quiet:
        warnings.filterwarnings("ignore")

    # get detection and annotation files
    if args.det_dir is None:
        args.det_dir = args.files
    if args.ann_dir is None:
        args.ann_dir = args.files
    det_files = search_files(args.det_dir, args.det_suffix)
    ann_files = search_files(args.ann_dir, args.ann_suffix)
    # quit if no files are found
    if len(ann_files) == 0:
        print "no files to evaluate. exiting."
        exit()

    # list to collect the individual evaluation objects
    eval_objects = []

    # evaluate all files
    num_files = len(ann_files)
    for num_file, ann_file in enumerate(ann_files):
        # print progress
        sys.stderr.write('\revaluated %d of %d files' %
                         (num_file + 1, num_files))
        sys.stderr.flush()

        # load the annotations
        annotations = args.load_fn(ann_file)

        #  get the matching detection files
        matches = match_file(ann_file, det_files,
                             args.ann_suffix, args.det_suffix)
        if len(matches) > 1:
            # exit if multiple detections were found
            raise SystemExit("multiple detections for %s found" % ann_file)
        elif len(matches) == 0:
            # ignore non-existing detections
            if args.ignore_non_existing:
                continue
            # output a warning if no detections were found
            warnings.warn(" can't find detections for %s" % ann_file)
            # but continue and assume no detections
            detections = None
        else:
            # load the detections
            detections = args.load_fn(matches[0])

        # combine the annotations if needed
        if args.combine > 0:
            annotations = combine_events(annotations, args.combine)

        # shift the detections if needed
        if args.delay != 0:
            try:
                # try to shift the first column of a 2D array
                detections[:, 0] += args.delay
            except IndexError:
                # if that fails, it must be a 1D array already
                detections += args.delay

        # remove detections and annotations that are within the first N seconds
        if args.skip > 0:
            # skipping the first few seconds alters the results
            start_idx = np.searchsorted(detections, args.skip, 'right')
            detections = detections[start_idx:]
            start_idx = np.searchsorted(annotations, args.skip, 'right')
            annotations = annotations[start_idx:]

        # evaluate
        e = args.eval(detections, annotations, name=os.path.basename(ann_file),
                      **vars(args))

        # add this file's evaluation to the global evaluation list
        eval_objects.append(e)

    # clear progress information
    # TODO: is there a more elegant way of clearing the progress line?
    sys.stderr.write("\r                                                  \r")
    sys.stderr.flush()

    # output every evaluation object individually
    out_list = []
    if args.verbose:
        out_list.extend(eval_objects)
    # add sum/mean evaluation to output
    if args.sum_eval is not None:
        out_list.append(args.sum_eval(eval_objects))
    out_list.append(args.mean_eval(eval_objects))

    # output everything
    print args.output_formatter(out_list)


if __name__ == '__main__':
    main()
